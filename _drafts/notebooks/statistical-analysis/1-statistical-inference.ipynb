{
 "cells": [
  {
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "Statistical learning refers to a vast set of tools for understanding data:\n",
    "\n",
    "+ supervised: building a statistical model for predicting, or estimating, an output based on one or more inputs.\n",
    "+ unsupervised: learning relationships between inputs (no supervising output).\n",
    "\n",
    "This series of articles will focus on supervised learning.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# MODELING\n",
    "## Estimate function\n",
    "\n",
    "Suppose that we observe a quantitative response $Y$ and $p$ different predictors $X = X_1, X_2,...,X_p$. We assume that there is some relationship between them, which can be written in the very general form: $Y = f(X) + \\epsilon$.\n",
    "\n",
    "+ $f$ is some fixed but unknown function of $X$.\n",
    "+ $\\epsilon$ is a random error term, which is independent of $X$ and has a mean of zero.\n",
    "\n",
    "We create an estimate $\\hat{f}$ that predicts $Y$: $\\hat{Y} = \\hat{f}(X)$. Choosing $\\hat{f}$ depends on the goal of the modelisation.\n",
    "\n",
    "## Predictions vs Inference\n",
    "\n",
    "When focusing on **predictions accuracy**, we are not overly concerned with the shape of $\\hat{f}$, as long as it yields accurate predictions for $Y$: we treat it as a black box.\n",
    "\n",
    "When focusing on **inference**, we want to understand the way that $Y$ is affected as $X$ changes, so we cannot treat $\\hat{f}$ as a black box:\n",
    "\n",
    "+ Which predictors are associated with the response? Which ones are the most important?\n",
    "+ What is the relationship between the response and each predictor: positive or negative?\n",
    "+ Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?\n",
    "\n",
    "There are three types of inference problems:\n",
    "\n",
    "+ Estimate the value of a parameter. The estimation can either be:\n",
    "    + **point estimate**: a particular value that best approximates some parameter.\n",
    "    + **interval estimate**: interval of plausible values for the parameter. This can also include prediction intervals for future observations.\n",
    "\n",
    "+ Hypothesis Testing: yes/no answer as to whether a predictor is associated with the response.\n",
    "+ Relationship modelling: generalization of Hypothesis Testing to more than one predictor.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# SAMPLING\n",
    "\n",
    "We almost never have access the entire population of interest; we start with the subset of data that we were able to collect. Our goal is to make [generalizations](https://www.encyclopediaofmath.org/index.php/Statistical_inference) about unseen data based on this sample; we will build the estimate $\\hat{f}$ from the sample data and apply it to new values of $X$. \n",
    "\n",
    "The sample needs to represent the population well for our conclusions to be valid. To do this, the sample data needs to be **randomly generated** from the entire population.\n",
    "\n",
    "\n",
    "## Sampling Methods\n",
    "\n",
    "There are several ways to sample a population:\n",
    "\n",
    "+ [Simple random sample](https://en.wikipedia.org/wiki/Simple_random_sample) – each subject in the population has an equal chance of being selected. Some demographics might be missed.\n",
    "+ [Stratified random sample](https://en.wikipedia.org/wiki/Stratified_sampling) – the population is divided into groups based on some characteristic (e.g. sex, geographic region). Then simple random sampling is done for each group based on its size in the actual population.\n",
    "+ [Cluster sample](https://en.wikipedia.org/wiki/Cluster_sampling) – a random cluster of subjects is selected from the population (e.g. certain neighborhoods instead of the entire city).\n",
    "\n",
    "\n",
    "## Sampling bias\n",
    "\n",
    "There are several forms of [sampling bias](https://en.wikipedia.org/wiki/Sampling_bias) that can lead to incorrect inference:\n",
    "+ selection bias: not fully representative of the entire population.\n",
    "    + people who answer surveys.\n",
    "    + people from specific segments of the population (polling about health at fruit stand).\n",
    "+ survivorship bias: population improving over time by having lesser members leave due to death.\n",
    "    + head injuries with metal helmets increasing vs cloth caps because less lethal.\n",
    "    + damage in WWII planes: not uniformally distributed in planes that came back, but only in non-critical areas.\n",
    "\n",
    "_Note: other [criteria](https://en.wikipedia.org/wiki/Selection_bias) can also impact the representativity of our sample._\n",
    "\n",
    "\n",
    "## Limitations of Statistical Inference\n",
    "\n",
    "Due to the random nature of sampling, some samples are **not representative** of the population and will produce incorrect inference. This uncertainty is reflected in the **confidence level** of statistical conclusions:\n",
    "+ a small proportion of samples, typically noted $\\alpha$, will produce incorrect inferences.\n",
    "+ for 1 - $\\alpha$ percents of all samples, the conclusions will be correct.\n",
    "+ the confidence level is therefore expressed as 1 - $\\alpha$.\n",
    "\n",
    "_Note: 0.01 and 0.05 are the most common values of $\\alpha$. This translates to 99% and 95% confidence intervals._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# Frequentist vs Bayesian Paradigms\n",
    "\n",
    "Both paradigms are based on likelihood but their frameworks are entirely different.\n",
    "\n",
    "## Frequentist Paradigm\n",
    "\n",
    "In the frequentist paradigm, the **parameter** is **set but unknown**. \n",
    "\n",
    "Due to the random nature of sampling, some samples are not representative of the population. It means that a small proportion of samples, typically noted $\\alpha$, will produce incorrect inferences. This probability of errors can be controlled to build **(1 - $\\alpha$) Percent Confidence Intervals**. \n",
    "\n",
    "This means that for (1 - $\\alpha$) percents of all samples, the calculated interval will actually include the parameter value.\n",
    "\n",
    "_Note: this is not a probability. The interval either includes the parameter value or it doesn't._\n",
    "\n",
    "## Bayesian Paradigm\n",
    "\n",
    "In the Bayesian paradigm, the **parameter** is a **random variable**. \n",
    "\n",
    "It is assigned a **prior distribution** based on already available (prior) data. This distribution is updated by the likelihood of the sample values to obtain its **posterior distribution**. From it, both **point estimate** and **region of highest posterior density** (or credible intervals) can be derived.\n",
    "\n",
    "## Considerations\n",
    "\n",
    "A rigorous approach to frequentist statistics assume that the conditions of the experiment are well-defined even before any data is actually collected. \n",
    "\n",
    "Baysesian statistics, on the other hand, make no such assumptions. They are especially useful when new data is constently collected: our beliefs are constantly updated, older data being used as prior to the new data that comes in. \n",
    "\n",
    "We will cover Bayesian Statistics in-depth in another series of articles.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TODO - Further Reads\n",
    "\n",
    "A few interesting Wikipedia articles:\n",
    "\n",
    "Generalities\n",
    "+ https://en.wikipedia.org/wiki/Sampling_distribution\n",
    "+ https://en.wikipedia.org/wiki/Statistical_hypothesis_testing \n",
    "\n",
    "Probabilities\n",
    "+ https://en.wikipedia.org/wiki/Probability_interpretations\n",
    "+ https://en.wikipedia.org/wiki/Frequentist_probability\n",
    "+ https://en.wikipedia.org/wiki/Bayesian_probability\n",
    "\n",
    "Inference paradigms:\n",
    "+ https://en.wikipedia.org/wiki/Frequentist_inference\n",
    "+ https://en.wikipedia.org/wiki/Bayesian_inference\n",
    "+ https://en.wikipedia.org/wiki/Lindley%27s_paradox\n",
    "+ https://www.stat.berkeley.edu/~stark/Preprints/611.pdf\n",
    "\n",
    "PArametric vs Ordinal\n",
    "+ https://tech.snmjournals.org/content/46/3/318.2#:~:text=Currie%20writes%2C%20%E2%80%9CThe%20Likert%20scale,the%20data%20ordinal%20in%20nature.&text=Moreover%2C%20he%20concludes%20that%20parametric,distribution%20of%20data)%20are%20violated.\n",
    "+ https://www.researchgate.net/post/What_is_the_most_suitable_statistical_test_for_ordinal_data_eg_Likert_scales\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TODO - LIMITS OF SUMMARY STATISTICS - ANSCOMBES QUARTET\n",
    "\n",
    "[Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) comprises four data sets of eleven data points (see below) that have nearly identical descriptive statistics, yet have very different distributions and appear very different when [graphed](https://matplotlib.org/3.2.1/gallery/specialty_plots/anscombe.html). They were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data before analyzing it and the effect of outliers and other influential observations on statistical properties.\n",
    "\n",
    "1. simple linear relationship with gaussian noise. \n",
    "1. clear non-linear relationship between variables; the Pearson correlation coefficient is not relevant here. A more general regression and the corresponding [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) would be more appropriate. \n",
    "1. the relationship is linear but  one outlier has enough influence to offset the calculated regression; it lowers the correlation coefficient from 1 to 0.816. A [robust regression](https://en.wikipedia.org/wiki/Robust_regression) would be more appropriate here.\n",
    "1. example when one [high-leverage point](https://en.wikipedia.org/wiki/Leverage_(statistics)) is enough to produce a high correlation coefficient, even though the other data points do not indicate any relationship between the variables.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data \n",
    "x = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\n",
    "y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n",
    "y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\n",
    "y3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n",
    "x4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\n",
    "y4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n",
    "\n",
    "datasets = {\n",
    "    'I': (x, y1),\n",
    "    'II': (x, y2),\n",
    "    'III': (x, y3),\n",
    "    'IV': (x4, y4)\n",
    "}\n",
    "\n",
    "# create fig\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4, sharex=True, sharey=True, figsize=(15, 4))\n",
    "x_lin = np.array([np.min(x+x4), np.max(x+x4)])\n",
    "\n",
    "for ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n",
    "\n",
    "    # linear regression\n",
    "    p1, p0 = np.polyfit(x, y, deg=1)\n",
    "    y_lin = p1 * x_lin + p0\n",
    "\n",
    "    # plot\n",
    "    ax.plot(x, y, 'o')\n",
    "    ax.plot(x_lin, y_lin, 'r-', alpha=0.5, lw=2)\n",
    "\n",
    "    # add title\n",
    "    ax.set_title(label)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 0.9])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "aae8ee8694c59c72b15b6d3f40b32b92958c6b1dcd63d116167f24907625a630"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}