{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we observe a quantitative response $Y$ and $p$ different predictors $X = X1, X2,...,Xp$. We assume that there is some relationship between them, which can be written in the very general form: \n",
    "\n",
    "$$Y = f(X) + \\epsilon$$\n",
    "\n",
    "+ $f$ is some fixed but unknown function of $X$.\n",
    "+ $\\epsilon$ is a random error term, which is independent of $X$ and has a mean of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an estimate $\\hat{f}$ that predicts $Y$: $\\hat{Y} = \\hat{f}(X)$. There will always be two errors elements:\n",
    "\n",
    "$$E (Y - \\hat{Y})^2 = [f(X) - \\hat{f}(X)]^{2} + Var(\\epsilon)$$\n",
    "\n",
    "Where:\n",
    "+ $E (Y - \\hat{Y})^2$ is the average squared error of predictions.\n",
    "+ $[f(X) - \\hat{f}(X)]^{2} $ is the reducible error. Our aim is to reduce this error.\n",
    "+ $Var(\\epsilon)$ is the irreducible error, that cannot be predicted using $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions vs Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When focusing on **predictions accuracy**, we are not overly concern with the shape of $\\hat{f}$, as long as it yields accurate predictions for $Y$: we treat it as a black box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When focusing on **inference**, we want to understand the way that $Y$ is affected as $X$ changes, so we cannot treat $\\hat{f}$ as a black box:\n",
    "\n",
    "+ Which predictors are associated with the response? Which ones are the most important?\n",
    "+ What is the relationship between the response and each predictor: positive or negative? Is there covariance?\n",
    "+ Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of Fit - Bias vs Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good model **accurately predicts** the desired target value for **new data**. It will have:\n",
    "+ low **bias**: how well the model approximates the data.\n",
    "+ low **variance**: how stable the model is in response to new training examples.\n",
    "\n",
    "We can link under- vs overfitting to bias and variance:\n",
    "+ the **underfitting** model does not capture the relevant relations between features and outputs: it suffers from **high bias**.\n",
    "+ the **overfitting** model captures the underlying noise in the training set, so changing the training set will lead to vastly different predictions: it suffers from **high variance**.\n",
    "\n",
    "The figure below illustrates the range of predictions for a given input by a model trained with different datasets, depending on its bias and variance *([source](http://scott.fortmann-roe.com/docs/BiasVariance.html))*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastienplat.s3.amazonaws.com/a9a3a238b8b5a0bfe07d83b1f07c85bd1472143621831\" align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more detailed article about the [bias-variance tradeoff](https://sebastienplat.github.io/blog/bias-variance-tradeoff) and how to identify under- and overfitting models is available on this blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Parametric vs Non-Parametric Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We make an **assumption about the functional form**, or shape, of $f$, the simplest of which is that it is linear.\n",
    "2. We fit the model to a training set. It finds the values of the function's parameters that match $Y_{train}$ more closely.\n",
    "\n",
    "Example for a linear model:\n",
    "1. $f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p$.\n",
    "2. Find values of $\\beta_0, ..., \\beta_p$ that minimizes the gaps between $\\hat{Y}_{train}$ and $Y_{train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The potential disadvantage of a parametric approach is that:\n",
    "+ the model we choose will usually **not match the true unknown form of $f$**. If the chosen model is too far from the true $f$, then our estimate will be poor **(underfitting)**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple parametric models will **not work** if the number of features is **close to or higher than** the number of observations (more details [here](https://medium.com/@jennifer.zzz/more-features-than-data-points-in-linear-regression-5bcabba6883e)). These cases will require a different approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Parametric Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-parametric methods **do not make explicit assumptions about the functional form** of $f$. Instead they seek an estimate of $f$ that gets as close to the data points as possible, without being too rough.\n",
    "\n",
    "While non-parametric approaches avoid the issues of parametrics assumptions, they suffer from a major disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a **very large number of observations** (far more\n",
    "than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$. It can also **follow the noise** too closely **(overfitting)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Linear models** allow for relatively **simple and interpretable** inference, but may not yield as accurate predictions as some other approaches. \n",
    "+ Highly **non-linear** approaches may provide predictions that are **more accurate**, but this comes at the expense of **less interpretability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using categorical predictors requires some preparation. The most typical method is called one-hot encoding; it creates dummy boolean variables for all but one category: 1 if the observation has this category and 0 otherwise. The remaining category is called the baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A succesful model has low bias and low variance, which means it will accurately predict out-of-sample performance. \n",
    "\n",
    "There are two main approaches to train and test a model:\n",
    "+ splitting the dataset in **three subsets**: one for training, one for validation and one for final testing.\n",
    "+ splitting the dataset in multiple train/test sets: **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Validation / Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the dataset for training the model:\n",
    "\n",
    "+ randomly shuffle data, in order to remove any bias that could stem from the original ordering of the dataset.\n",
    "+ split the dataset into training and testing subsets. Using a `random_state` ensures the split is always the same.\n",
    "+ a typical split is 80% of the observations for the training set and 20% for the test set.\n",
    "\n",
    "This method risks overfitting the test set in case of [multiple iterations](https://glassboxmedicine.com/2019/09/15/best-use-of-train-val-test-splits-with-tips-for-medical-data/) (bleeding). A more robust method is to keep an **holdout test set** completely untouched during the entire iteration process. This holdout set will be used only at the very last stage of the process, to assess the accuracy of the final model on completely unseen data.\n",
    "\n",
    "A typical split is 70% of the observations for the training set, 15% for the validation set and 15% for the holdout set. The drawback is that more data is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method can be used to assess both bias and variance of a given model: **K-fold cross-validation**.\n",
    "+ the dataset is divided in K subsets of equal size called \"folds\".\n",
    "+ the model is trained K times, each run holding out a different fold as test set.\n",
    "+ the average testing score is used as an estimate of out-of-sample performance.\n",
    "    \n",
    "This is especially useful for hyperparameters tuning, as it avoids combinations that overfit the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=550px align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about Cross-Validation can be found [here](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ \n",
    "\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are great for inference. They help explain:\n",
    "+ if there is a linear relationship between variables.\n",
    "+ how strong is the relationship between variables.\n",
    "+ which variable(s) have a stronger impact on the outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main assumptions of linear models describe the relationship between predictors and response.\n",
    "+ **additive**: the effect of changes in a predictor on the response is independent from the values of the others.\n",
    "+ **linear**: one unit change of a given predictor leads to a constant change in the response. \n",
    "\n",
    "These highly restrictive assumptions that are often violated in practice, which requires more flexible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships with Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficients t-statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the errors are Gaussian, we can build a **confidence interval** for each coefficient of our model (in a way that is very similar to the confidence interval of the sample mean). \n",
    "\n",
    "This allows us to test the **null hypothesis** that the true value of each coefficient is zero, i.e. that there is **no relationship** between a given variable and the outcome, given the estimated coefficient value and the resulting standard error. \n",
    "\n",
    "We can calculate the **p-value** of the related **t-statistic**: how likely such a substantial association between the predictor and the response would be observed due to chance. Having a estimated value larger than the related standard error means that this probability is very small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model's F-statistic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test the **null hypothesis** that the true value of all coefficients is zero, i.e. that there is **no relationship** between the predictors and the outcome. This hypothesis can be assessed by the **p-value** of the model's **F-statistic**.\n",
    "\n",
    "_Note: the squared t-statistic of each coefficient is the F-statistic of a model that omits that variable. So it reports the partial effect of adding that variable to the model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Individual p-values vs Model p-value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some p-values will be below the significance level by chance. This means that using individual t-statistics and associated p-values will probably lead to the incorrect conclusion that there is a relationship, especially if the number of variables is high.\n",
    "\n",
    "The F-statistic, on the other hand, adjusts for the number of predictors. It means that it only 5% chance of Type-I error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the p-value of the F-statistic shows that some of our model's variables are related to the outcome, the next steps will be to identify which ones are important. There are a few classical methods to do it, if $p$ is the total number of predictors:\n",
    "\n",
    "+ **Forward selection**: start with the null model (no predictors). Fit $p$ simple linear models and keep the one with the lowest residual errors. Keep adding variables one by one until some condition is met. \n",
    "+ **Backward selection**: start with all predictors. Remove the one with the highest p-values. Keep removing variables until some condition is met (all remaining p-values under some threshold).\n",
    "+ **Mixed selection**: start with forward selection until the highest p-value crosses some threshold, then remove the predictor. Iterate until all p-values are below some threshold and adding any other predictors would lead to high p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Backward selection cannot be used if p>n, while forward selection can always be used. Forward selection is a greedy approach, and might include variables early that later become redundant. Mixed selection can remedy this._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of a linear model is usually assessed using two related quantities:\n",
    "+ the residual squared error (RSE).\n",
    "+ the $R^2$ statistic.\n",
    "\n",
    "Roughly speaking, the RSE is the average amount that the response will deviate from the true regression line. It is considered a measure of the lack of fit of the model.\n",
    "\n",
    "$R^2$ is the proportion of variance in outcomes explained by the model. It is always between 0 and 1; the closer it is to 1, the better the fit. When the model only includes one variable, $R^2$ is equal to the squared correlation coefficient $r^2$.\n",
    "\n",
    "_Note: if the data is inherently noisy, or outcomes are influenced by unmeasured factors, $R^2$ might be very small._\n",
    "\n",
    "_Note: Measures of fit include Mallow’s Cp, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted R2._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting residuals** can help identifying patterns not captured by the model, like **interactions between predictors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are associated with three categories of uncertainty:\n",
    "+ coefficients estimates.\n",
    "+ model bias due to distance of reality from linear assumptions.\n",
    "+ random errors due to noise.\n",
    "\n",
    "These uncertainties can be quantified to provide:\n",
    "+ confidence intervals: average outcome given specific values of the predictors.\n",
    "+ prediction intervals: outcome for a given observation given specific values of the predictors.\n",
    "\n",
    "Prediction intervals tend to be much wider than confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Improving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ large sample size, few features: a flexible model would fit the data better; the large sample size will limit the overfitting.\n",
    "+ small sample size, large amount of features: a flexible model would probably overfit the training set.\n",
    "+ large variance of the error term: a flexible model would probably capture the noise and generalize poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error will also be high. Potential solutions:\n",
    "\n",
    "+ Add new features.\n",
    "+ Add more complexity by introducing polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error will be much lower than test error. Potential solutions:\n",
    "\n",
    "+ Increase training size.\n",
    "+ Reduce number of features, especially those with weak signal to noise ratio.\n",
    "+ Increase Regularization terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Regularization Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization aims to prevent overfitting by penalizing large weights when training the model. It adds a regularization term to the loss function, with a regularization parameter called $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization - LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ L1 regularization penalizes the absolute value of the weights. \n",
    "+ It can do feature selection: insignificant input features are assigned a weight of zero.\n",
    "+ The resulting models are simple and interpretable, but cannot learn complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization - Ridge Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ L2 regularization penalizes the square of the weights. \n",
    "+ It forces the weights to be small but not zero.\n",
    "+ Taking squares into account makes it sensititive to outliers.\n",
    "+ It is able to learn complex data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*See also [this link](https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2) for more information.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few methods can be [used](https://scikit-learn.org/stable/modules/feature_selection.html) to reduce the number of features and decrease the risks of overfitting:\n",
    "\n",
    "+ Remove features with low variance.\n",
    "+ Univariate selection: only keep features that correlate highly with the outcome feature.\n",
    "+ Regressive feature elimination: only keep the features that lead to the most accurate model in CV. \n",
    "+ LASSO: only keep features with non-null weigths.\n",
    "+ Tree-based features importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on hyperparameters tuning can be found [here](https://scikit-learn.org/stable/modules/grid_search.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)** technique, we define a range of values for every parameter we want to tune. The Grid Search will train models for each combination of values using K-fold CV, then outputs the compared performances.\n",
    "\n",
    "This technique can become VERY resource-intensive for large datasets. In might be better to use **[RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)** in those instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Appendix - Further Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [Introduction to Statistical Learning](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) _(link to downlad the .pdf version)_\n",
    "+ [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) _(github.io based e-book)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ bias / variance: what / impact / how to measure?\n",
    "+ loss function: how to define?\n",
    "+ cross-validation: when / purpose?\n",
    "+ R² vs adjusted-R²: when / good choice?\n",
    "+ R² vs adjusted-R² on random noise\n",
    "\n",
    "+ feature importance: train or test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to double-check:\n",
    "\n",
    "+ https://en.m.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
    "+ http://scott.fortmann-roe.com/docs/MeasuringError.html\n",
    "+ http://cs229.stanford.edu/materials/ML-advice.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "323px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
