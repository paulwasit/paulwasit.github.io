{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38032bit64a64ed7a47843b8be3706a54e9a0958",
   "display_name": "Python 3.8.0 32-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "___\n",
    "\n",
    "# HYPOTHESIS TESTING\n",
    "\n",
    "## Goal \n",
    "\n",
    "Understanding relationships between response and predictors is about testing two opposite hypotheses:\n",
    "+ null hypothesis $H_0$: there is no relationship between a predictor and a response.\n",
    "+ alternative hypothesis: there is a relationship.\n",
    "\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "Hypothesis testing is used to **make decisions about a population** using sample data. \n",
    "\n",
    "+ We start with a **null hypothesis $H_0$** that we we asssume to be true. For instance:\n",
    "    + the sample parameter is equal to a given value.\n",
    "    + samples with different characteristics are drawn from the same population.\n",
    "+ We run an **experiment** to test this hypothesis:\n",
    "    + **collect data** from a sample of predetermined size.\n",
    "    + perform the appropriate **statistical test**.\n",
    "+ Based on the experimental results, we can either **reject** or **fail to reject** this null hypothesis. \n",
    "+ If we reject it, we say that the data supports another, mutually exclusive, **alternate hypothesis**.\n",
    "\n",
    "\n",
    "## Test Distribution\n",
    "\n",
    "A statistical test is based on some assumption regarding the sampling distribution of the test statistics under the null hypothesis; The assumptions depend on the test. \n",
    "\n",
    "\n",
    "## P-value\n",
    "\n",
    "We measure the probability that a sample would have a test statistics at least as extreme as the one observed, if these assumptions (and thus the null hypothesis) were true. This probability is called the **p-value**. \n",
    "\n",
    "To draw conclusions, we use a predetermined cutoff probability called the level of significance $\\alpha$ (typically 5%). \n",
    "\n",
    "+ $\\text{p-value }\\leq\\alpha$: the observed data is very unlikely under the null hypothesis so we reject it. The observed effect is statistically significant.\n",
    "+ $\\text{p-value }\\gt\\alpha$: we fail to reject the null hypothesis. The observed effect is not statistically significant.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# PARAMETERS OF A TEST\n",
    "\n",
    "A statistical test has several important parameters:\n",
    "+ alpha (Type I Error): probability of false positive.\n",
    "+ beta (Type II Error): probability of false negative.\n",
    "+ power (1 - beta): probability of true negative (correcting failing to reject the null hypothesis).\n",
    "\n",
    "\n",
    "## Types of Errors\n",
    "\n",
    "There are four possible outcomes for our hypothesis testing, with two [types of errors](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors):\n",
    "\n",
    "| Decision          | $$H_0$$ is True                      | $$H_0$$ is False                     |\n",
    "|-------------------:|:---------------------------------:|:---------------------------------:|\n",
    "| **Reject H0** | **Type I error**: False Positive   | Correct inference: True Positive |\n",
    "| **Fail to reject H0** | Correct inference: True Negative | **Type II error**: False Negative |\n",
    "\n",
    "_Note: Decreasing the Type I error increases the probability of the Type II error._\n",
    "\n",
    "\n",
    "### Type I error\n",
    "\n",
    "The Type I error is the probability of incorrecly rejecting the null hypothesis when the sample belongs to the population but with extreme values; this probability is equal to the level of significance $\\alpha$. It is also called False Positive: falsely stating that the alternate hypothesis is true.\n",
    "\n",
    "\n",
    "### Type II error\n",
    "\n",
    "The Type II error $\\beta$ is the probability of incorrectly failing to reject a null hypothesis; it is also called False Negative.\n",
    "\n",
    "\n",
    "## Statistical Power\n",
    "\n",
    "[Power](https://en.wikipedia.org/wiki/Statistical_power), also called sensitivity, is the probability of correctly rejecting a false $H_0$; It is equal to $1 - \\beta$.\n",
    "\n",
    "Calculating the power helps asserting whether we can confidently fail to reject the null hypothesis (i.e. large p-values). A low power means that the effect might be too small to detect in the conditions of our test.\n",
    "\n",
    "Two key things impact statistical power:\n",
    "+ the effect size: a large difference between groups is easier to detect.\n",
    "+ the sample size: it directly impacts the test statistic and the p-value.\n",
    "\n",
    "\n",
    "## P-Value vs Errors\n",
    "\n",
    "The p-value is linked to both error types:\n",
    "\n",
    "+ alpha is the maximum p-value we consider low enough to safely reject $H_0$.\n",
    "+ power is the probability we correctly fail to reject $H_0$ when the p-value is above alpha.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Bonferroni Corrections\n",
    "\n",
    "The chance of capturing rare event increases when testing multiple hypothesis. It means the likelihood of incorrectly rejecting a null hypothesis (false positive) increases. \n",
    "\n",
    "The Bonferroni correction rejects the null hypothesis for each $p_{i} \\leq \\frac {\\alpha}{m}$. This ensures the [Family Wise Error Rate](https://en.wikipedia.org/wiki/Family-wise_error_rate) stays below the significance level $\\alpha$. More information can be found [here](https://stats.stackexchange.com/questions/153122/bonferroni-correction-for-post-hoc-analysis-in-anova-regression).\n",
    "\n",
    "It is useful for post-hoc tests after performing one-way ANOVA or Chi-Square tests that reject the null hypothesis. When comparing $N$ multiple groups, we can either do:\n",
    "+ pairwise testing. In that case, $m$ will be ${N \\choose 2}$.\n",
    "+ one vs the rest. In that case, $m$ will be $N$.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# PARAMETRIC vs NON-PARAMETRIC\n",
    "\n",
    "Outliers?\n",
    "\n",
    "References:\n",
    "+ discussions around the importance of normality assumptions can be found [here](https://www.annualreviews.org/doi/pdf/10.1146/annurev.publhealth.23.100901.140546) and [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6676026/).\n",
    "+ discussions around t-tests vs non-parametric tests can be found [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445820/#B5) and [here](https://www.contemporaryclinicaltrials.com/article/S1551-7144(09)00109-8/fulltext).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# EFFECT SIZE vs SAMPLE SIZE\n",
    "## Effect Size\n",
    "\n",
    "The effect size quantifies how large is the difference in outcome is due to the input variable.\n",
    "\n",
    "+ continuous outcome in relation to discrete factors (t-tests & ANOVA): Cohen's d (difference of means divided by pooled standard deviation).\n",
    "+ discrete variables: odd ratio (strength of association between events).\n",
    "+ continuous variables: correlation coefficient.\n",
    "\n",
    "\n",
    "## Choosing the right Sample Size\n",
    "\n",
    "Given the variance of data $\\sigma$ and the minimum difference to detect $\\delta$, a typical formula to assess [sample size](https://en.wikipedia.org/wiki/Sample_size_determination) is:\n",
    "\n",
    "$$N = (z_\\alpha + z_\\beta)^2 \\times \\frac{\\sigma^2}{\\delta^2}$$\n",
    "\n",
    "Where $z_\\alpha$ and $z_\\beta$ are the z-score of $\\alpha$ and $\\beta$, respectively. \n",
    "\n",
    "You need to collect data for the sample of size $N$ calculated above before being able to draw conclusions.\n",
    "\n",
    "In a normal distribution, 95% of the data is between --2 and +2 standard deviations from the mean. Even for skewed data, going two standard deviations away from the mean often captures nearly all of the data.\n",
    "\n",
    "If we know the minimum and maximum values that the population is likely to take (excluding outliers), we can suppose they represent this interval of four standard deviations.\n",
    "\n",
    "It means the standard deviation of a population $\\sigma$ can be approximated by:\n",
    "\n",
    "$\\sigma \\simeq 1/4 \\times \\Delta_{range}$\n",
    "\n",
    "If we know the margin of error $E$ we are ready to accept at $1 - \\alpha$ confidence, the sample size we need can be approximated by:\n",
    "\n",
    "$n \\simeq [Z_{\\alpha/2} \\times \\sigma / E]^2 \\simeq [Z_{\\alpha/2} \\times  \\Delta_{range} / 4 E]^2 $\n",
    "\n",
    "A more accurate method to estimate the sample size: iteratively evaluate the following formula, until the $n$ value chosen to calculate the t-value matches the resulting $n$.\n",
    "\n",
    "$n \\simeq [t_{\\alpha/2, n-1} \\times  \\Delta_{range} / 4 E]^2 $\n",
    "\n",
    "\n",
    "## Large samples\n",
    "\n",
    "Tests become more sensitive with large sample sizes (i.e. they can capture smaller variations). Applying small-sample statistical inference to large samples means that even minuscule effects can become statistically significant.\n",
    "\n",
    "> The question is not whether differences are ‘significant’ (they nearly always are in large samples), but whether they are interesting. Forget statistical significance, what is the practical significance of the results?\n",
    "\n",
    "*More details can be found in this [academic paper](https://pdfs.semanticscholar.org/262b/854628d8e2b073816935d82b5095e1703977.pdf/).*\n",
    "\n",
    "More useful links:\n",
    "+ https://statmodeling.stat.columbia.edu/2009/06/18/the_sample_size/\n",
    "+ https://stats.stackexchange.com/questions/2516/are-large-data-sets-inappropriate-for-hypothesis-testing\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Appendix - Further Reads\n",
    "\n",
    "A few interesting Wikipedia articles:\n",
    "\n",
    "Generalities\n",
    "+ https://en.wikipedia.org/wiki/Sampling_distribution\n",
    "+ https://en.wikipedia.org/wiki/Statistical_hypothesis_testing \n",
    "\n",
    "Probabilities\n",
    "+ https://en.wikipedia.org/wiki/Probability_interpretations\n",
    "+ https://en.wikipedia.org/wiki/Frequentist_probability\n",
    "+ https://en.wikipedia.org/wiki/Bayesian_probability\n",
    "\n",
    "Inference paradigms:\n",
    "+ https://en.wikipedia.org/wiki/Frequentist_inference\n",
    "+ https://en.wikipedia.org/wiki/Bayesian_inference\n",
    "+ https://en.wikipedia.org/wiki/Lindley%27s_paradox\n",
    "+ https://www.stat.berkeley.edu/~stark/Preprints/611.pdf\n",
    "\n",
    "PArametric vs Ordinal\n",
    "+ https://tech.snmjournals.org/content/46/3/318.2#:~:text=Currie%20writes%2C%20%E2%80%9CThe%20Likert%20scale,the%20data%20ordinal%20in%20nature.&text=Moreover%2C%20he%20concludes%20that%20parametric,distribution%20of%20data)%20are%20violated.\n",
    "+ https://www.researchgate.net/post/What_is_the_most_suitable_statistical_test_for_ordinal_data_eg_Likert_scales\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TODO - Topics\n",
    "\n",
    "+ Power & Significance & Effect size\n",
    "\n",
    "+ t-test\n",
    "+ linear regression & link with t-test\n",
    "+ ANOVA\n",
    "+ Chi-square\n",
    "+ F-statistic\n",
    "\n",
    "https://www.annualreviews.org/doi/pdf/10.1146/annurev.publhealth.23.100901.140546\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TODO\n",
    "\n",
    "Can we use common distributions to approximate the sampling distribution of the test statistic & under what conditions.\n",
    "\n",
    "_Note: many statistical methods assume the data is roughly normal. This assumption must always be checked first: many things that you might assume are normally distributed are actually not. In particular, outliers are extremely unlikely for normally distributed data; if your data does have extreme values, the normal distribution might not be the best description._\n",
    "\n",
    "_Note: We can compare the ECDF to the theoritical CDF of the normal distribution with same mean and standard deviation to assess if the data is normally distributed._\n",
    "\n",
    "_Note: If the data distribution is far from normal, using a t-test that focuses on the mean [might not be the most relevant test](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6676026/)._\n",
    "\n",
    "It the t-distribution cannot be used, it is possible to use a more robust procedure such as the one-sample [**Wilcoxon procedure**](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test).\n",
    "\n",
    "\n",
    "+ skewness / kurtosis\n",
    "+ mean/ std vs boxplots & outliers\n",
    "\n",
    "\n",
    "Sometimes, we need to summarize the data in one or two numbers. The most common one for continuous data are the mean or the median.\n",
    "\n",
    "+ mean, based on the values of the data, is heavily influenced by outliers.\n",
    "+ median, based on the ranking of the values, is immune to outliers.\n",
    "\n",
    "The spread of data is measured by the standard deviation from the mean.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}