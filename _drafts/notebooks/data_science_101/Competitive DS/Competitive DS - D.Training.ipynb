{
 "cells": [
  {
   "source": [
    "# TOC\n",
    "\n",
    "+ Bias vs Variance\n",
    "+ Validation\n",
    "+ Training Curves\n",
    "+ Hyperparameter Tuning\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Validation\r\n",
    "\r\n",
    "Using a validation set will limit the risk of overfitting our model on the training set. Once we are satisfied with a given model, we can retrain it on the entire dataset with the same hyperparameters.\r\n",
    "\r\n",
    "Of particular interest will be how the sample data was extracted from the database: was it random, or were some classes over-sampled to produce a more balanced dataset. This will be crucial to set up a proper validation scheme: when splitting the training set to create the validation set, we want the resulting split to be as close to the split between training and test set as possible. \r\n",
    "\r\n",
    "The scikit-learn documentation has a section dedicated to [validation](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. Validation Methods\r\n",
    "\r\n",
    "How to split / what ratios to use:\r\n",
    "+ holdout (`sklearn.model_selection.ShuffleSplit`): split the dataset in two and train only on one part. This is a good choice when the dataset is large or when the model score is likely to be consistent across splits.\r\n",
    "+ K-fold (`sklearn.model_selection.Kfold`): split dataset into K subsets; train K times on K-1 subsets. This method ensures that a given sample is used for validation only once. The end score is the average over these K-folds. This is a good choice when the dataset is of medium size  \r\n",
    "+ Leave-one-out (`sklearn.model_selection.LeaveOneOut`): K-fold where K is equal to the number of samples in the training set. This is a good choice when the dataset is small and the models are relatively fast to train. \r\n",
    "\r\n",
    "_Note: it is important to ensure that there is no overlap between training and validation sets, ehich could occur if there are duplicate samples. In this case, the accuracy of the validation set risks being incorrectly high._\r\n",
    "\r\n",
    "_Note: with K-fold and LOO, you can also estimate mean and variance of the loss. This can be used to measure if improvements are statistically signiffication**\r\n",
    "\r\n",
    "For classification datasets that are eith small, or with a large amount of categories, it is always a good practice to use stratification: this method ensures that the distribution of classes will be similar over different training folds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Inconsistent Scores \n",
    "\n",
    "There are two main reasons for observing vastly different scores for our different folds:\r\n",
    "+ the data has clear patterns but little data. The model will not be able to generalize them well. Each fold train on slightly different patterns, which can lead to vastly different scores.\r\n",
    "+ the data is inconsistent. In this case where the variance is high, the model will struggle to generalize.\r\n",
    "\r\n",
    "Running several K-folds with different seeds can help get a better estimate of the model's performance. It can also be helpful to adjust hyperparameters with one seed and estimate performance with another one.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3. Competition-specific Steps\r\n",
    "A few extra steps can be taken during competitions to compare v*Validation scores vs leaderboard scor:**+ \n",
    "\n",
    "Sometimes, the validation score is very different from the leaderboard one. It usually comes from using a validation set that is not representative of the test set. If the test set distribution is different from the validation test, doing some learderboard probing to get mean values can help improve the score significantly.+ \n",
    "\n",
    "A good practicforro final submissions is to submit one model that performs well in the validation set (to cover cases where test set et validation set have similar distributions) and one that performs well on the public leaderboard (to cover cases where the distribution of the test set if very different from the one of the validation s)\n",
    "s."
   ]
  },
  {
   "source": [
    "# II. Training Curves\n",
    "\n",
    "TODO.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# III. Hyperparameters Tuning\n",
    "\n",
    "Understanding the effects of each hyperparameter allow us to select the ones to tune in order to address either situations of under- and overfitting.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## III.1. GBDT\n",
    "\n",
    "There are three GBDT algorithms that will benefit from these technics: XGBoost, LightBGM and CatBoost. Scikit-Learn has RandomForest and ExtraTrees.\n",
    "\n",
    "| XGboost                                | LightGBM                                   | RandomForest        | Description                                                                                                          | Impact | Good starting value |\n",
    "|----------------------------------------|--------------------------------------------|---------------------|----------------------------------------------------------------------------------------------------------------------|--------|---------------------|\n",
    "| max_depth                              | + max_depth + num_leaves                   | max_depth           | Max depth of a tree and leaves per level. The optimal value is typically higher for Random Forests than for GBDT.    | +      | 7                   |\n",
    "| subsample                              | bagging_fraction                           |                     | Fraction of objects to use when fitting a tree.                                                                      | +      |                     |\n",
    "| + colsample_bytree + colsample_bylevel | feature_fraction                           | max_features        | Fraction of features to use when fitting a tree.                                                                     | +      |                     |\n",
    "| + min_child_weight + lambda + alpha    | + min_data_in_leaf + lambda_l1 + lambda_l2 | min_samples_leaf    | Regularization parameters. min_child_weight has the biggest impact and is one of the most important hyperparameters. | -      | 1 - 300             |\n",
    "| + eta + num_rounds                     | + learning_rate + num_iterations           | + NA + N_estimators | Learning rate (similar to gradient descent) and number of learning steps (i.e. trees) to build.                      |        |                     |\n",
    "| seed                                   | *_seed                                     | random_state        |                                                                                                                      |        |                     |\n",
    "|                                        |                                            | criterion           | For Random Forest Classifiers only. Either Gini or Entropy.                                                          |        |                     |\n",
    "|                                        |                                            | n_jobs              | Indicate the number of CPU cores to use for training (default is 1).                                                 |        |                     |\n",
    "\n",
    "**Max Depth**\n",
    "\n",
    "Increasing the max depth will lead to longer training times, so it's better to do it only when necessary.\n",
    "\n",
    "If increasing the depth of trees does not lead to overfitting, it means that there is a lot of important information to extract from the data; In this case, it might be usefulto stop tuning and try to generate new features.\n",
    "\n",
    "**Subsmaple**\n",
    "\n",
    "Reducing the fraction of objects to use for each tree might reduce overfitting. It's akin to a regularization parameter.\n",
    "\n",
    "\n",
    "**Learning Rate**\n",
    "\n",
    "Large learning rates will lead the model to fit faster but is prone to overfitting. And a learning rate that is to large will not converge so the model won't fit. Smaller learning rates lead to less overfitting but a learning rate that is too small will learn nothing even after many rounds.\n",
    "\n",
    "We can start by using a relatively small learning rate, say 0.1 or 0.01, and use early stopping to find the number of iterations it takes for the model to overfit. We can then divide the learning rate and multiply the number of rounds by the same amount to improve the model's performance.\n",
    "\n",
    "\n",
    "Contrary to GBDT that build trees one after the other, Random Forests build trees independently. It means that having many trees does not lead to overfitting. So the first step is to identify the number of trees (N_estimators) that is sufficient for the problem at hand. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 32-bit",
   "name": "python38032bit64a64ed7a47843b8be3706a54e9a0958"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}