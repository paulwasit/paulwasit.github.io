{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38032bit64a64ed7a47843b8be3706a54e9a0958",
   "display_name": "Python 3.8.0 32-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MODULES\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import chisquare\n"
   ]
  },
  {
   "source": [
    "___\n",
    "\n",
    "# STATISTICAL TESTS\n",
    "## Choosing a Statistical Test\n",
    "Choosing a [statistical test](http://www.biostathandbook.com/) depends on:\n",
    "+ what hypothesis is tested.\n",
    "+ the type of the variable of interest & its probability distribution.\n",
    "\n",
    "In some simple cases, we do not have to explicitely model relationships and can use specific statistical tests instead; The most common example is the t-test to compare two samples. As we'll see in the next article, these statistical tests are particular cases of more general linear models. For example, t-tests are a linear model where X = \"sample the observation belongs to\". We try to assess if there is a relationship between X and Y, or if the response depends on the sample.\n",
    "\n",
    "More generally, choosing a [statistical test](http://www.biostathandbook.com/) depends on what we want to measure:\n",
    "\n",
    "<br></br>\n",
    "![png](../../img/stat_tests/stat_tests_overview.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# INFERENCE\n",
    "## Overview\n",
    "\n",
    "We can infer the value of the **population parameter** based on the sample statistics. Which parameter represents the population the best depends on the probability distribution.\n",
    "\n",
    "_Fisher & Chi-Squared tests measure the proportion of samples in different categories._\n",
    "\n",
    "<br></br>\n",
    "![png](../../img/stat_tests/stat_tests_inference.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# DIFFERENCE BETWEEN SAMPLES\n",
    "## Overview\n",
    "\n",
    "Comparing samples aims to determine if some characteristics of the population have an impact on the variable of interest. More specifically, we check if different values of some **categorical variable(s)** lead to **different probability distributions** for the variable of interest.\n",
    "\n",
    "<br></br>\n",
    "![png](../../img/stat_tests/stat_tests_diff_between_samples.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# CORRELATION BETWEEN VARIABLES\n",
    "## Overview\n",
    "\n",
    "Correlation is the measure of dependance between **two continuous or ordinal variables**; It typically indicates their linear relationship, but more broadly measures how in sync they vary. This is expressed by their **covariance**. \n",
    "\n",
    "A more common measure is the **[Pearson product-moment correlation coefficient](https://en.wikipedia.org/wiki/Correlation_and_dependence#Pearson's_product-moment_coefficient)**, built on top of the covariance. It's akin to the standard variation vs the variance for bivariate data and represents how far the relationship is from the line of best fit.\n",
    "\n",
    "The correlation coefficient divides the covariance by the product of the standard deviations. This normalizes the covariance into a unit-less variable whose values are between -1 and +1.\n",
    "\n",
    "The line of best fit has a slope equal to the Pearson coefficient multiplied by SDy / SDx.\n",
    "\n",
    "<br></br>\n",
    "![png](../../img/stat_tests/stat_tests_correlation.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# MODELING\n",
    "## Overview\n",
    "\n",
    "Linear Regression:\n",
    "+ only incude variables that are correlated to the outcome.\n",
    "+ check for collinearity.\n",
    "\n",
    "<br></br>\n",
    "![png](../../img/stat_tests/stat_tests_modeling.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TODO - Topics\n",
    "\n",
    "+ linear regression & link with t-test\n",
    "+ ANOVA\n",
    "+ Chi-square\n",
    "+ F-statistic\n",
    "\n",
    "https://www.annualreviews.org/doi/pdf/10.1146/annurev.publhealth.23.100901.140546\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# TESTING FOR NORMALITY\n",
    "## Parametric Assumptions of Normality\n",
    "\n",
    "T-tests and ANOVA assume that all the samples are normally distributed. Testing for normality is required for small samples but not for large ones, as the mean of large samples is close to the population mean. Testing for normality can be done with the Shapiro-Wilk test (or visually with QQ-plots).\n",
    "\n",
    "Non-parametric tests can be used when\n",
    "+ the assumption of normality is not met. \n",
    "+ the **mean** is **not the most appropriate** parameter to describe the population.\n",
    "\n",
    "They are less sensitive than parametric tests, which means their chances of true positives is lower and chances of false negatives are higher.\n",
    "\n",
    "_Note: many statistical methods assume the data is roughly normal. This assumption must always be checked first: many things that you might assume are normally distributed are actually not. In particular, outliers are extremely unlikely for normally distributed data; if your data does have extreme values, the normal distribution might not be the best description._\n",
    "\n",
    "_Note: We can compare the ECDF to the theoritical CDF of the normal distribution with same mean and standard deviation to assess if the data is normally distributed._\n",
    "\n",
    "References:\n",
    "+ discussions around the importance of normality assumptions can be found [here](https://www.annualreviews.org/doi/pdf/10.1146/annurev.publhealth.23.100901.140546) and [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6676026/).\n",
    "+ discussions around t-tests vs non-parametric tests can be found [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445820/#B5) and [here](https://www.contemporaryclinicaltrials.com/article/S1551-7144(09)00109-8/fulltext).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TODO\n",
    "\n",
    "Can we use common distributions to approximate the sampling distribution of the test statistic & under what conditions.\n",
    "\n",
    "_Note: many statistical methods assume the data is roughly normal. This assumption must always be checked first: many things that you might assume are normally distributed are actually not. In particular, outliers are extremely unlikely for normally distributed data; if your data does have extreme values, the normal distribution might not be the best description._\n",
    "\n",
    "_Note: We can compare the ECDF to the theoritical CDF of the normal distribution with same mean and standard deviation to assess if the data is normally distributed._\n",
    "\n",
    "_Note: If the data distribution is far from normal, using a t-test that focuses on the mean [might not be the most relevant test](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6676026/)._\n",
    "\n",
    "It the t-distribution cannot be used, it is possible to use a more robust procedure such as the one-sample [**Wilcoxon procedure**](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test).\n",
    "\n",
    "\n",
    "+ skewness / kurtosis\n",
    "+ mean/ std vs boxplots & outliers\n",
    "\n",
    "\n",
    "Sometimes, we need to summarize the data in one or two numbers. The most common one for continuous data are the mean or the median.\n",
    "\n",
    "+ mean, based on the values of the data, is heavily influenced by outliers.\n",
    "+ median, based on the ranking of the values, is immune to outliers.\n",
    "\n",
    "The spread of data is measured by the standard deviation from the mean.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Q-Q Plots\n",
    "\n",
    "### Construction\n",
    "\n",
    "Q–Q (quantile-quantile) plots compare two probability distributions by plotting their quantiles against each other. They are commonly used to compare a dataset to a theoretical model, providing a graphical assessment of \"goodness of fit\" rather than a numerical summary.\n",
    "\n",
    "_Note: When the two datasets have the same size, the Q_Q plot orders each set in increasing order and pairs off the corresponding values. Otherwise, it is necessary to interpolate quantile estimates for the smallest dataset._\n",
    "\n",
    "_Note: in an ordered sample, the kth-smallest value is called its [kth-order statistic](https://en.wikipedia.org/wiki/Order_statistic). For the normal distribution, tje order statistics are called [rankits](https://en.wikipedia.org/wiki/Rankit)._\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "If the two distributions being compared are similar, the points in the Q–Q plot will approximately lie on the line y = x. If the distributions are linearly related, the points in the Q–Q plot will approximately lie on a line, but not necessarily on the line y = x.\n",
    "\n",
    "A non-linear pattern suggests the two datasets don't have the same probability distribution.\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "The examples below show PDFs, CDFs and QQ-plots for a few common distributions. Note that the PDF of actual samples can only be approximated by histograms and Kernel Density Estimates, so the CDF will be easier to analyze.\n",
    "\n",
    "![qqplot](../../img/qq-plot.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Shapiro-Wilk Test\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Jarque-Bera Test\n",
    "\n",
    "The Jarque–Bera test is a goodness-of-fit test based on the sample skewness and kurtosis: its null hypothesis states that both skewness and excess kurtosis are zero. If the data comes from a normal distribution, the JB test statistic asymptotically follows a chi-squared distribution with two degrees of freedom.\n",
    "\n",
    "The chi-squared approximation is overly sensitive for small samples, often rejecting the null hypothesis when it is true (large Type I error rate). This is why this test is only recommended for large samples (n > 2000).\n",
    "\n",
    "_Note: some implementations interpolate p-values for small samples via Monte-Carlo simulations, in order to account for discrepancies between calculations and true alpha values._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# TESTING THE NULL HYPOTHESIS\n",
    "\n",
    "We can formulate three possible **alternate hypotheses**:\n",
    "\n",
    "Test          | Left-tailed      | Two-sided          | Right-tailed        |\n",
    "-------------:|-----------------:|:------------------:|:-------------------:|\n",
    "One-sample    | $\\mu - \\mu_0 \\lt 0$  | $\\mu - \\mu_0 \\neq 0$   | $\\mu - \\mu_0 \\gt 0$     |\n",
    "Two-sample    | $\\mu_1 - \\mu_2 \\lt 0$  | $\\mu_1 - \\mu_2 \\neq 0$   | $\\mu_1 - \\mu_2 \\gt 0$     |\n",
    "\n",
    "We calculate the probability of observing the sample statistic $t^*$ under $H_0$. Depending on the alternate hypothesis and significance level $\\alpha$, we reject $H_0$ if $t^*$ is in the blue areas of the **sampling distribution of sample statistic**:\n",
    "\n",
    "<img class=\"center-block\" src=\"https://sebastienplat.s3.amazonaws.com/21a0a7a855f51f6426dfbf6115b872161490032937519\"/>\n",
    "\n",
    "_Note: for two-tailed tests, we use $\\alpha/2$ for each tail and $2*t^*$ as the p-value. This ensures the total probability of extreme values is $\\alpha$._\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TODO - Bonferroni Corrections\n",
    "\n",
    "The chance of capturing rare event increases when testing multiple hypothesis. It means the likelihood of incorrectly rejecting a null hypothesis (false positive) increases. \n",
    "\n",
    "The Bonferroni correction rejects the null hypothesis for each $p_{i} \\leq \\frac {\\alpha}{m}$. This ensures the [Family Wise Error Rate](https://en.wikipedia.org/wiki/Family-wise_error_rate) stays below the significance level $\\alpha$. More information can be found [here](https://stats.stackexchange.com/questions/153122/bonferroni-correction-for-post-hoc-analysis-in-anova-regression).\n",
    "\n",
    "It is useful for post-hoc tests after performing one-way ANOVA or Chi-Square tests (explained in the next chapters) that reject the null hypothesis. When comparing $N$ multiple groups, we can either do:\n",
    "+ pairwise testing. In that case, $m$ will be ${N \\choose 2}$.\n",
    "+ one vs the rest. In that case, $m$ will be $N$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## T-Tests\n",
    "\n",
    "In its most common form, a t-test **compare means** of continuous variables. Both one-tail and two-tailed alternate hypothesis are possible.\n",
    "\n",
    "+ one-sample null hypothesis: the mean of a population has a specific value.\n",
    "+ two-sample null hypothesis: the means of two populations are equal.\n",
    "+ dependent two-sample null hypothesis: the mean of a sample is unchanged after some event.\n",
    "\n",
    "Different versions of t-tests exist to handle different situations:\n",
    "+ same sample size, equal variance.\n",
    "+ different sample size, equal variance.\n",
    "+ different variance (Welch's t-test).\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "T-tests make the following **assumptions**:\n",
    "+ the sample **mean(s)** follow a **normal distribution** (this is always the case for large samples under the CLT).\n",
    "+ the sample **variance(s)** follow a **$\\chi^2$ distribution** (this is always the case for normally distributed data).\n",
    "\n",
    "In practice, t-tests can be used when:\n",
    "+ the sample size **is large** (30+ observations), OR\n",
    "+ the **population** is roughly **normal** (very small samples - use normal probability plots to assess normality).\n",
    "\n",
    "It the t-distribution cannot be used, it is possible to use a more robust procedure such as the one-sample [**Wilcoxon procedure**](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test).\n",
    "\n",
    "_Note: If the data distribution is far from normal, using a t-test that focuses on the mean [might not be the most relevant test](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6676026/)._\n",
    "\n",
    "\n",
    "### T-statistic\n",
    "\n",
    "A t-statistic will be larger (i.e. less likely to happen by chance) if:\n",
    "+ the compared statistic values are very different.\n",
    "+ the pooled standard deviation is small, ie. the compared distributions do not overlap much.\n",
    "+ the samples are large.\n",
    "\n",
    "\n",
    "### One-Sample T-Test\n",
    "\n",
    "The t-test is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis.\n",
    "\n",
    "A t-test is most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known. When the scaling term is unknown and is replaced by an estimate based on the data, the test statistics (under certain conditions) follow a Student's t distribution. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___\n",
    "\n",
    "# USEFUL DISTRIBUTIONS\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}